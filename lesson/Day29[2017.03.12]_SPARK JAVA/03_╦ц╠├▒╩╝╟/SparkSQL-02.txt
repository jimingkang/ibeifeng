


针对于SparkCore或者SparkSQL进行的数据统计分析，一般情况下，会使用MR和HIVE去验证，HQL语句。


Spark 核心编程
	-1,SparkCore					- MapReduce
		SparkContext
			绝大多数的情况下处理的数据来源于HDFS
			sc.textFile("")
		RDD
			RDD#Transformation
			RDD#Action
	-2,SparkSQL						- Hive
		数据：
			在企业中大数据的数据来源与Hive表
				大数据集
			维度数据
				RDBMS的表中 jdbc
		功能：
			HiveQL、DSL
		SQLContext
			sqlContext.read
				.text()
				.table()
				.json()
				.jdbc()
				.parquet()
			dataframe#func
			sqlContext.sql("")
			dataframe.write
				.text()
				.jdbc()
				.table()
		DataFrame			
			等价于关系型数据库的二维表，或者Python中的DataFrame
	-3,SparkStreaming
		StreamingContext
		DStream
			分离、离散
			DStream#Transformation
			DStream#Output
Spark 高级编程
	-1,Spark MLlib
	-2,SparkGraphX
	-3,SparkR
		R 数据分析的工具，主要用于实际企业项目中，而是在于科研
	-4,Python、Spark
	
	
大数据平台技术发展：
	-1,MySQL + Python
		SQL 语句
	-2,Hive + Python
		进行的过滤加载，分析处理
https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-MovieLensUserRatings
		SQL
	-3,HIVE + Spark
		SQL
		
	Kylin
大数据工程师
	爬虫
	-1,基于Java的框架
		Nutch
	-2,基于Python的框架
		Scripy

=========================================================
SparkSQL中的自定函数
	-1,UDF
	    // 案例一、定义UDF：一对一
		sqlContext.udf.register(
		  "toUpper" ,// function name
		  (word: String) => word.toUpperCase()   // 匿名函数
		)
	-2,UDAF
		多对一
		聚合函数
		求各个部门的平均工资
			avg(sal)
		案例：
			如何获取平均数
				总和 / 总数
			缓冲数据
				sal_total = 0 + 5000.0  + 2450.0  + 1300.0 = 8750.0
				sal_count = 0 +  1      +  1      +  1     = 3
			最终的输出数据
				sal_total / sal_count 
		分析：
			-1,定义输入数据类型
			-2,依据需求定义出 缓冲数据类型
				字段、类型
			-3,每条数据进入函数以后
				缓冲数据的变化
			-4,对所有分区数据的合并处理
				分布式计算
			-5,最终的结果数据
				
		
雇员表EMP，有如下字段：deptno, sal，需求：获取各个部门的平均工资
sqlContext.sql("SELECT deptno, double_scala(avg(sal), 2) AS aa, double_scala(sal_avg(sal), 2) AS bb FROM db_hive.emp GROUP BY deptno").show
+------+------------------+
|deptno|               _c1|
+------+------------------+
|    10|2916.6666666666665|
|    20|            2175.0|
|    30|1566.6666666666667|
+------+------------------+
	注册函数：
sqlContext.udf.register(
  "sal_avg", 
  AvgSalUDAF
)


sqlContext.udf.register(
  "double_scala",
  (numValue: Double, scale: Int) => {
    import java.math.BigDecimal
    val db = new BigDecimal(numValue)
    db.setScale(scale, BigDecimal.ROUND_HALF_UP).doubleValue()
  }
)

============================================================
SparkSQL
	Spark2.x
Dataset
	另外一个数据的抽象，类似于RDD或者类似于DataFrame，从Spark2.0开始SparkSQL的API都是Dataset。
	
A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.).

Dataset与RDD
	a specialized Encoder to serialize the objects for processing or transmitting over the network.
		使用特定的Encoder进行序列化和反序列化对象
创建两种方式
	方式一：
		从本地集合创建
scala> val list = List(1, 2, 3, 4, 5)
list: List[Int] = List(1, 2, 3, 4, 5)

scala> val ds = list.toDS()
ds: org.apache.spark.sql.Dataset[Int] = [value: int]		

	方式二：
		从DataFrame创建
		DataFrame=Dataset[Row]
scala> val df = sqlContext.read.json("/datas/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> case class People(age: Long, name: String)

scala> val ds_people = df.as[People]
ds_people: org.apache.spark.sql.Dataset[People] = [name: string, age: bigint]














Spark 2.x
	SCALA:
		2.11.x
	MAVEN:
		3.3.3+
	IDAE:
		15+













	
	
	
	
	
	
